server:
  error:
    include-message: always
    include-binding-errors: always
  port: ${PORT:8083}

kafka:
  properties:
    num-partitions: 1
    replication-factor: 1
  topic:
    product: "product"
    order: "order"
    reservation: "reservation"
    payment: "payment"

spring:
  kafka:
    # Required connection configs for Kafka producer, consumer, and admin
    bootstrap-servers: ${BOOTSTRAP_SERVERS}
    # Required connection configs for Confluent Cloud Schema Registry
    properties:
      stream-properties:
        application-id: order-ms
        schema.registry.url: ${SCHEMA_HOST}
        application.id: order-ms
        bootstrap.servers: ${BOOTSTRAP_SERVERS}
      bootstrap-servers: ${BOOTSTRAP_SERVERS}
      security.protocol: SASL_SSL
      sasl.mechanism: PLAIN
      credentials.source: USER_INFO
      sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username=${KAFKA_USER} password='${KAFKA_PASS}';
      schema:
        registry:
          url: ${SCHEMA_HOST}
          basic:
            auth:
              user:
                info: ${SCHEMA_INFO}
      basic:
        auth:
          credentials:
            source: USER_INFO
            user:
              info: ${INFO_PASS}
    consumer:
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringSerializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      properties:
        auto.offset.reset: earliest
        specific.avro.reader: true
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
    streams:
      properties:
        application.id: order-ms
        bootstrap.servers: ${BOOTSTRAP_SERVERS}
        schema.registry.url: ${SCHEMA_HOST}
        default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        default.value.serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
        num.standby.replicas: 1
        num.stream.threads: 2
        max.request.size: 8388608        # 84 MiB
        topic.max.message.bytes: 8388608 # 84 MiB
        topology.optimization: all
        state.dir: /tmp/kafka-streams
      application-id: order-ms
  output.ansi.enabled: ALWAYS
  cloud:
    discovery:
      enabled: false
  datasource:
    url: jdbc:h2:mem:product-app;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE
    driverClassName: org.h2.Driver

  jpa:
    database: POSTGRESQL
    show-sql: false
    generate-ddl: false
    hibernate:
      ddl-auto: update
    database-platform: org.hibernate.dialect.PostgreSQL9Dialect
    properties:
      open-in-view: false
      hibernate:
        temp:
          use_jdbc_metadata_defaults: false
      datasource:
        url: jdbc:postgresql://localhost:5432/order
        username: postgres
        password: secset

logging:
  pattern.console: "%clr(%d{HH:mm:ss.SSS}){blue} %clr(---){faint} %clr([%15.15t]){yellow} %clr(:){red} %clr(%m){faint}%n"
  level:
    org.apache.kafka.streams.kstream: INFO
    org.springframework.data: DEBUG
    org.springframework.boot.autoconfigure: ERROR  
    org.hibernate: DEBUG