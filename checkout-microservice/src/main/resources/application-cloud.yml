server:
  port: ${PORT:8084}

kafka:
  topic:
    order: "book-order"

spring:
  application:
    name: checkout-microservice
  kafka:
    # Required connection configs for Kafka producer, consumer, and admin
    bootstrap-servers: ${BOOTSTRAP_SERVERS}
    # Required connection configs for Confluent Cloud Schema Registry
    properties:
      stream-properties:
        schema.registry.url: ${SCHEMA_HOST}
        bootstrap.servers: ${BOOTSTRAP_SERVERS}
      bootstrap-servers: ${BOOTSTRAP_SERVERS}
      security.protocol: SASL_SSL
      sasl.mechanism: PLAIN
      credentials.source: USER_INFO
      sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username=${KAFKA_USER} password='${KAFKA_PASS}';
      schema:
        registry:
          url: ${SCHEMA_HOST}
          basic:
            auth:
              user:
                info: ${SCHEMA_INFO}
      basic:
        auth:
          credentials:
            source: USER_INFO
            user:
              info: ${INFO_PASS}
    consumer:
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        auto.offset.reset: earliest
        specific.avro.reader: true
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    streams:
      replication-factor: 1
      properties:
        application.id: checkout-microservice
        bootstrap.servers: ${BOOTSTRAP_SERVERS}
        schema.registry.url: ${SCHEMA_HOST}
        default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        default.value.serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
        num.standby.replicas: 1
        num.stream.threads: 2
        max.request.size: 8388608        # 84 MiB
        topic.max.message.bytes: 8388608 # 84 MiB
        topology.optimization: all
        state.dir: /tmp/kafka-streams
  output.ansi.enabled: ALWAYS
  jpa:
    database: POSTGRESQL
    show-sql: true
    generate-ddl: true
    hibernate:
      ddl-auto: create-drop
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    properties:
      hibernate:
        temp:
          use_jdbc_metadata_defaults: false
    open-in-view: false
  datasource:
    url: jdbc:postgresql://localhost:5432/postgres
    username: postgres
    password: postgres
logging:
  pattern.console: "%clr(%d{HH:mm:ss.SSS}){blue} %clr(---){faint} %clr([%15.15t]){yellow} %clr(:){red} %clr(%m){faint}%n"
  level:
    
    org.springframework.data: DEBUG
    org.springframework.boot.autoconfigure: ERROR
    org.hibernate: DEBUG